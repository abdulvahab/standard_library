{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Over the last few newsletters, I have been showing you how the \"multiprocessing\" module makes it possible not only to use processes in a way that feels like you're using threads, but also to share data using a Queue. That's right -- while we normally think of queues as having to do with threads, and giving us a thread-safe mechanism for passing data among threads, it turns out that \"multiprocessing\" offers its own version of a queue, which we can use quite similarly.\n",
    "\n",
    "Queues depend on Python's \"pickle\" module, which allows us to turn any Python object into a string. In other languages, this is known as \"serialization\" or \"marshalling,\" but for reasons that are somewhat beyond me, Python people talk about \"pickling.\"  All built-in data structures (not including things like functions and files) can be pickled and unpickled. We can store a pickled object to a string, or we can store it to disk.  And of course, we can unpickle a string or disk file into an object. Moreover, pickle works with your own classes, unless they do truly strange things. \n",
    "\n",
    "The good news, then, is that you can basically send any kind of Python-based data structure through a queue.\n",
    "\n",
    "The bad news is that if you're passing lots of data, then the process of pickling and unpickling might incur some overhead. Also, queues are designed for sending data from one process to another. What if more than two processes want to share data?  What if you have lots of pieces of data that need to be updated or monitored regularly?\n",
    "\n",
    "If we were working within a single process, we could use a shared piece of data to handle such things. But when we have multiple processes, the point is that each process has its own variables and memory space, and is insulated from the other processes.\n",
    "\n",
    "One solution is to use the \"shared memory\" feature of the \"multiprocessing\" module. This feature relies on shared memory capabilities within the operating system, allowing processes to communicate with one another.\n",
    "\n",
    "Since we're going through the operating system, we won't have the flexibility of using Python data types, as provided by our queue. Instead, we'll be restricted to several low-level data types, corresponding to individual values and arrays at the C level. The types that are available to us are defined in the \"ctypes\" library, which describes the ways in which Python can use C-compatible data structures.\n",
    "\n",
    "For example, let's assume that we want to count the size of a number of files. As we've seen before, we can launch a new process from \"multiprocessing\" to open a file and get its size. When we've done this in the past, we've placed each file's size in a queue; when all of the processes are done, we can then retrieve and add all of the values from the queue.\n",
    "\n",
    "Using shared memory, we can have each process increment the value. No counting will be necessary afterwards, although we will want to \"join\" the processes in order to ensure that all of them have finished.\n",
    "\n",
    "In this example, I create a new \"Value\" object in the main process that launches everything. The \"Value\" object needs to be given a type and a value, much like a variable declaration in C or any other static language. In this case, I've given it an \"i\" type (i.e., integer) and an initial value of 0.  To us, \"word_count\" is an integer like any other. However, \"multiprocessing\" keeps track of it across processes, such that it's visible elsewhere.\n",
    "\n",
    "We can then update it from each individual process, as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from multiprocessing import Process, Value\n",
    "import glob\n",
    "\n",
    "def count_words(counter, filename):\n",
    "    try:\n",
    "        counter.value += len(open(filename).read())\n",
    "    except IOError:\n",
    "        print(\"\\tProblem with {}\".format(filename))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    word_count = Value('i', 0)\n",
    "\n",
    "    processes = [ ]\n",
    "    for filename in glob.glob('/etc/*.conf'):\n",
    "        print(filename)\n",
    "        p = Process(target=count_words, args=(word_count,filename))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for one_process in processes:\n",
    "        one_process.join()\n",
    "\n",
    "    print(\"Total = {}\".format(word_count.value))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our main program code is in an \"if __name__ == '__main__'\" block, which is there in part to keep things organized, and in part because Windows and Unix have such different approaches to launching processes that this is necessary to work on both.  We create our new \"Value\" object, we create a list into which we can store our processes, we launch the processes (one for each matching file in \"glob\"), and then join them up.  In the end, we have the final value -- without the need for queues.\n",
    "\n",
    "But wait a second: How do we know that the processes aren't somehow colliding when they try to update the word_count variable?  It's probably a good idea to use a mutex or other form of lock to ensure that only one person can modify the variable at once -- especially since multiple processes have the potential to do lots of trouble.\n",
    "\n",
    "You might think that += provides a sort of lock. But it doesn't: +=, like other operators in Python, is actually transformed into a method call.  In this case, that call would be to the __iadd__ method, which doesn't lock our Value object throughout its changes. This means that if enough processes are updating our count at the same time, you might find that the count isn't working correctly. \n",
    "\n",
    "We can get around this by adding a single line: \n",
    "\n",
    "    with counter.get_lock():\n",
    "to the start of count_words. Our values, it would seem, are more versatile than simple numeric storage would be.  They have the \"get_lock\" method, which -- when put in a context manager (\"with\") block, automatically do precisely what we would like: Make sure that no one else is using the variable, then update it atomically, and then release the lock.\n",
    "\n",
    "The updated software can thus look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from multiprocessing import Process, Value\n",
    "import glob\n",
    "\n",
    "def count_words(counter, filename):\n",
    "    with counter.get_lock():\n",
    "        try:\n",
    "            counter.value += len(open(filename).read())\n",
    "        except IOError:\n",
    "            print(\"\\tProblem with {}\".format(filename))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    word_count = Value('i', 0)\n",
    "\n",
    "    processes = [ ]\n",
    "    for filename in glob.glob('/etc/*.conf'):\n",
    "        print(filename)\n",
    "        p = Process(target=count_words, args=(word_count,filename))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for one_process in processes:\n",
    "        one_process.join()\n",
    "\n",
    "    print(\"Total = {}\".format(word_count.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can have an entire \"Array\" of values, rather than a single \"Value\" object. The \"ctypes\" manual page lists many types you can specify.\n",
    "\n",
    "    https://docs.python.org/3/library/ctypes.html#module-ctypes\n",
    "\n",
    "Next time, we'll talk about process pools -- and then finally wrap up this very (!) long series I've been writing about threads and processes.\n",
    "\n",
    "Until then,\n",
    "\n",
    "Reuven"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
